{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Healthcare Data Analysis - Understanding Disease Risk\n",
        "\n",
        "## Project Overview\n",
        "This notebook provides a comprehensive analysis of healthcare datasets to understand disease risk factors. We'll analyze patterns in health features like blood pressure, cholesterol, sugar levels, BMI, age, and other factors to identify which contribute most to disease risk.\n",
        "\n",
        "## Table of Contents\n",
        "1. [Data Loading and Initial Exploration](#1-data-loading-and-initial-exploration)\n",
        "2. [Data Preprocessing](#2-data-preprocessing)\n",
        "3. [Exploratory Data Analysis (EDA)](#3-exploratory-data-analysis-eda)\n",
        "4. [Risk Factor Analysis](#4-risk-factor-analysis)\n",
        "5. [Statistical Analysis](#5-statistical-analysis)\n",
        "6. [Insights and Recommendations](#6-insights-and-recommendations)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import warnings\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import requests\n",
        "import io\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading and Initial Exploration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_healthcare_data():\n",
        "    \"\"\"\n",
        "    Load healthcare dataset from local file or download from UCI repository\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Try to load diabetes dataset first\n",
        "        df = pd.read_csv('diabetes.csv')\n",
        "        print(\"‚úÖ Diabetes dataset loaded successfully!\")\n",
        "        return df, 'diabetes'\n",
        "    except FileNotFoundError:\n",
        "        try:\n",
        "            # Try heart disease dataset\n",
        "            df = pd.read_csv('heart_disease.csv')\n",
        "            print(\"‚úÖ Heart disease dataset loaded successfully!\")\n",
        "            return df, 'heart_disease'\n",
        "        except FileNotFoundError:\n",
        "            print(\"üì• No local dataset found. Downloading sample diabetes dataset...\")\n",
        "            \n",
        "            # Download diabetes dataset from UCI ML Repository\n",
        "            url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
        "            \n",
        "            # Column names for diabetes dataset\n",
        "            columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', \n",
        "                     'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
        "            \n",
        "            try:\n",
        "                response = requests.get(url)\n",
        "                df = pd.read_csv(io.StringIO(response.text), names=columns)\n",
        "                df.to_csv('diabetes.csv', index=False)\n",
        "                print(\"‚úÖ Diabetes dataset downloaded and saved successfully!\")\n",
        "                return df, 'diabetes'\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error downloading dataset: {e}\")\n",
        "                print(\"Please ensure you have a healthcare dataset (diabetes.csv or heart_disease.csv) in the project directory.\")\n",
        "                return None, None\n",
        "\n",
        "# Load the dataset\n",
        "df, dataset_type = load_healthcare_data()\n",
        "\n",
        "if df is not None:\n",
        "    print(f\"\\nüìä Dataset Type: {dataset_type.title()}\")\n",
        "    print(f\"üìè Dataset Shape: {df.shape}\")\n",
        "    print(f\"\\nüîç First 5 rows:\")\n",
        "    display(df.head())\n",
        "else:\n",
        "    print(\"‚ùå Could not load dataset. Please check your data files.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if df is not None:\n",
        "    # Basic dataset information\n",
        "    print(\"üìã Dataset Information:\")\n",
        "    print(f\"‚Ä¢ Total Records: {len(df):,}\")\n",
        "    print(f\"‚Ä¢ Total Features: {len(df.columns)}\")\n",
        "    print(f\"‚Ä¢ Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "    print(f\"‚Ä¢ Missing Values: {df.isnull().sum().sum()}\")\n",
        "    print(f\"‚Ä¢ Duplicate Records: {df.duplicated().sum()}\")\n",
        "    \n",
        "    print(\"\\nüìä Data Types:\")\n",
        "    print(df.dtypes)\n",
        "    \n",
        "    print(\"\\nüìà Statistical Summary:\")\n",
        "    display(df.describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_data(df):\n",
        "    \"\"\"\n",
        "    Comprehensive data preprocessing function\n",
        "    \"\"\"\n",
        "    print(\"üîß Starting data preprocessing...\")\n",
        "    \n",
        "    # Create a copy to avoid modifying original\n",
        "    df_clean = df.copy()\n",
        "    \n",
        "    # 1. Handle missing values\n",
        "    print(\"\\n1Ô∏è‚É£ Handling missing values...\")\n",
        "    missing_before = df_clean.isnull().sum().sum()\n",
        "    \n",
        "    if missing_before > 0:\n",
        "        print(f\"   Found {missing_before} missing values\")\n",
        "        \n",
        "        # For numerical columns, fill with median\n",
        "        numerical_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
        "        for col in numerical_cols:\n",
        "            if df_clean[col].isnull().sum() > 0:\n",
        "                median_val = df_clean[col].median()\n",
        "                df_clean[col].fillna(median_val, inplace=True)\n",
        "                print(f\"   ‚Ä¢ {col}: filled {df_clean[col].isnull().sum()} missing values with median ({median_val:.2f})\")\n",
        "        \n",
        "        # For categorical columns, fill with mode\n",
        "        categorical_cols = df_clean.select_dtypes(include=['object']).columns\n",
        "        for col in categorical_cols:\n",
        "            if df_clean[col].isnull().sum() > 0:\n",
        "                mode_val = df_clean[col].mode()[0] if not df_clean[col].mode().empty else 'Unknown'\n",
        "                df_clean[col].fillna(mode_val, inplace=True)\n",
        "                print(f\"   ‚Ä¢ {col}: filled {df_clean[col].isnull().sum()} missing values with mode ({mode_val})\")\n",
        "    else:\n",
        "        print(\"   ‚úÖ No missing values found\")\n",
        "    \n",
        "    # 2. Remove duplicates\n",
        "    print(\"\\n2Ô∏è‚É£ Removing duplicates...\")\n",
        "    duplicates_before = df_clean.duplicated().sum()\n",
        "    if duplicates_before > 0:\n",
        "        df_clean = df_clean.drop_duplicates()\n",
        "        print(f\"   ‚Ä¢ Removed {duplicates_before} duplicate records\")\n",
        "    else:\n",
        "        print(\"   ‚úÖ No duplicates found\")\n",
        "    \n",
        "    # 3. Handle outliers using IQR method\n",
        "    print(\"\\n3Ô∏è‚É£ Handling outliers...\")\n",
        "    numerical_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
        "    outliers_removed = 0\n",
        "    \n",
        "    for col in numerical_cols:\n",
        "        if col in ['Outcome', 'target']:  # Skip target variables\n",
        "            continue\n",
        "            \n",
        "        Q1 = df_clean[col].quantile(0.25)\n",
        "        Q3 = df_clean[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        \n",
        "        outliers = df_clean[(df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)]\n",
        "        if len(outliers) > 0:\n",
        "            print(f\"   ‚Ä¢ {col}: found {len(outliers)} outliers (IQR method)\")\n",
        "            # Cap outliers instead of removing them\n",
        "            df_clean[col] = np.where(df_clean[col] < lower_bound, lower_bound, df_clean[col])\n",
        "            df_clean[col] = np.where(df_clean[col] > upper_bound, upper_bound, df_clean[col])\n",
        "            outliers_removed += len(outliers)\n",
        "    \n",
        "    if outliers_removed == 0:\n",
        "        print(\"   ‚úÖ No significant outliers found\")\n",
        "    else:\n",
        "        print(f\"   ‚Ä¢ Capped {outliers_removed} outlier values\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ Data preprocessing completed!\")\n",
        "    print(f\"   ‚Ä¢ Final dataset shape: {df_clean.shape}\")\n",
        "    print(f\"   ‚Ä¢ Memory usage reduced to: {df_clean.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "    \n",
        "    return df_clean\n",
        "\n",
        "# Apply preprocessing\n",
        "if df is not None:\n",
        "    df_processed = preprocess_data(df)\n",
        "    print(\"\\nüìä Processed dataset info:\")\n",
        "    display(df_processed.info())\n",
        "else:\n",
        "    print(\"‚ùå Cannot preprocess data - no dataset loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Exploratory Data Analysis (EDA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if df_processed is not None:\n",
        "    # Set up the plotting style\n",
        "    plt.style.use('default')\n",
        "    sns.set_palette(\"husl\")\n",
        "    \n",
        "    # 1. Distribution Analysis\n",
        "    print(\"üìä Distribution Analysis\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Get numerical columns (excluding target)\n",
        "    numerical_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
        "    target_cols = [col for col in numerical_cols if any(t in col.lower() for t in ['outcome', 'target'])]\n",
        "    feature_cols = [col for col in numerical_cols if col not in target_cols]\n",
        "    \n",
        "    # Create distribution plots\n",
        "    n_cols = 3\n",
        "    n_rows = (len(feature_cols) + n_cols - 1) // n_cols\n",
        "    \n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
        "    if n_rows == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "    \n",
        "    for i, col in enumerate(feature_cols):\n",
        "        row = i // n_cols\n",
        "        col_idx = i % n_cols\n",
        "        \n",
        "        axes[row, col_idx].hist(df_processed[col].dropna(), bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "        axes[row, col_idx].set_title(f'Distribution of {col}')\n",
        "        axes[row, col_idx].set_xlabel(col)\n",
        "        axes[row, col_idx].set_ylabel('Frequency')\n",
        "        axes[row, col_idx].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Hide unused subplots\n",
        "    for i in range(len(feature_cols), n_rows * n_cols):\n",
        "        row = i // n_cols\n",
        "        col_idx = i % n_cols\n",
        "        axes[row, col_idx].set_visible(False)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # 2. Correlation Analysis\n",
        "    print(\"\\nüîó Correlation Analysis\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    if len(numerical_cols) > 1:\n",
        "        corr_matrix = df_processed[numerical_cols].corr()\n",
        "        \n",
        "        plt.figure(figsize=(12, 8))\n",
        "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
        "                   square=True, mask=mask, fmt='.2f', cbar_kws={\"shrink\": .8})\n",
        "        plt.title('Correlation Matrix of All Features')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Show top correlations\n",
        "        print(\"\\nTop 10 Feature Correlations:\")\n",
        "        corr_pairs = []\n",
        "        for i in range(len(corr_matrix.columns)):\n",
        "            for j in range(i+1, len(corr_matrix.columns)):\n",
        "                corr_pairs.append((\n",
        "                    corr_matrix.columns[i], \n",
        "                    corr_matrix.columns[j], \n",
        "                    abs(corr_matrix.iloc[i, j])\n",
        "                ))\n",
        "        \n",
        "        corr_pairs.sort(key=lambda x: x[2], reverse=True)\n",
        "        for i, (col1, col2, corr) in enumerate(corr_pairs[:10]):\n",
        "            print(f\"{i+1:2d}. {col1} ‚Üî {col2}: {corr:.3f}\")\n",
        "    \n",
        "    # 3. Box Plots for Outlier Detection\n",
        "    print(\"\\nüì¶ Outlier Detection (Box Plots)\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    if len(feature_cols) > 0:\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "        df_processed[feature_cols].boxplot(ax=ax)\n",
        "        ax.set_title('Box Plots for Outlier Detection')\n",
        "        ax.set_xticklabels(feature_cols, rotation=45)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    # 4. Target Variable Analysis\n",
        "    if target_cols:\n",
        "        target_col = target_cols[0]\n",
        "        print(f\"\\nüéØ Target Variable Analysis: {target_col}\")\n",
        "        print(\"=\" * 50)\n",
        "        \n",
        "        # Target distribution\n",
        "        target_counts = df_processed[target_col].value_counts()\n",
        "        print(f\"Target Distribution:\")\n",
        "        for value, count in target_counts.items():\n",
        "            percentage = (count / len(df_processed)) * 100\n",
        "            print(f\"  {value}: {count} ({percentage:.1f}%)\")\n",
        "        \n",
        "        # Visualize target distribution\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "        \n",
        "        # Bar plot\n",
        "        target_counts.plot(kind='bar', ax=ax1, color=['lightcoral', 'lightblue'])\n",
        "        ax1.set_title(f'Distribution of {target_col}')\n",
        "        ax1.set_xlabel(target_col)\n",
        "        ax1.set_ylabel('Count')\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "        \n",
        "        # Pie chart\n",
        "        ax2.pie(target_counts.values, labels=target_counts.index, autopct='%1.1f%%', \n",
        "                colors=['lightcoral', 'lightblue'])\n",
        "        ax2.set_title(f'{target_col} Distribution (Percentage)')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    print(\"\\n‚úÖ EDA completed successfully!\")\n",
        "else:\n",
        "    print(\"‚ùå Cannot perform EDA - no processed dataset available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Risk Factor Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if df_processed is not None and target_cols:\n",
        "    print(\"üîç Risk Factor Analysis\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    target_col = target_cols[0]\n",
        "    feature_cols = [col for col in df_processed.select_dtypes(include=[np.number]).columns \n",
        "                   if col not in target_cols]\n",
        "    \n",
        "    # 1. Correlation with target\n",
        "    print(\"1Ô∏è‚É£ Feature Correlations with Target\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    correlations = df_processed[feature_cols + [target_col]].corr()[target_col].drop(target_col).abs().sort_values(ascending=False)\n",
        "    \n",
        "    print(\"Top Risk Factors (by correlation):\")\n",
        "    for i, (feature, corr) in enumerate(correlations.head(10).items(), 1):\n",
        "        print(f\"{i:2d}. {feature}: {corr:.3f}\")\n",
        "    \n",
        "    # Visualize correlations\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    correlations.plot(kind='barh', color='coral')\n",
        "    plt.title('Risk Factor Correlations with Target')\n",
        "    plt.xlabel('Absolute Correlation with Target')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # 2. Detailed analysis for top risk factors\n",
        "    print(\"\\n2Ô∏è‚É£ Detailed Risk Factor Analysis\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    top_factors = correlations.head(3).index\n",
        "    \n",
        "    for i, factor in enumerate(top_factors, 1):\n",
        "        print(f\"\\n{i}. {factor} Analysis:\")\n",
        "        print(\"=\" * 30)\n",
        "        \n",
        "        # Statistics by target\n",
        "        stats_by_target = df_processed.groupby(target_col)[factor].describe()\n",
        "        print(\"Statistics by Target:\")\n",
        "        display(stats_by_target)\n",
        "        \n",
        "        # Risk insights\n",
        "        mean_0 = df_processed[df_processed[target_col] == 0][factor].mean()\n",
        "        mean_1 = df_processed[df_processed[target_col] == 1][factor].mean()\n",
        "        diff = mean_1 - mean_0\n",
        "        \n",
        "        print(f\"\\nRisk Insights:\")\n",
        "        print(f\"‚Ä¢ Average {factor} for non-disease: {mean_0:.2f}\")\n",
        "        print(f\"‚Ä¢ Average {factor} for disease: {mean_1:.2f}\")\n",
        "        print(f\"‚Ä¢ Difference: {diff:.2f}\")\n",
        "        \n",
        "        if diff > 0:\n",
        "            print(\"‚Ä¢ Higher values increase disease risk\")\n",
        "        else:\n",
        "            print(\"‚Ä¢ Lower values increase disease risk\")\n",
        "        \n",
        "        # Create comparison plots\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "        \n",
        "        # Distribution by target\n",
        "        target_values = df_processed[target_col].unique()\n",
        "        colors = ['lightblue', 'lightcoral']\n",
        "        \n",
        "        for j, target_val in enumerate(target_values):\n",
        "            subset = df_processed[df_processed[target_col] == target_val]\n",
        "            ax1.hist(subset[factor].dropna(), alpha=0.7, \n",
        "                    label=f'Target = {target_val}', bins=20, color=colors[j])\n",
        "        \n",
        "        ax1.set_title(f'{factor} Distribution by Target')\n",
        "        ax1.set_xlabel(factor)\n",
        "        ax1.set_ylabel('Frequency')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "        \n",
        "        # Box plot\n",
        "        df_processed.boxplot(column=factor, by=target_col, ax=ax2)\n",
        "        ax2.set_title(f'{factor} by Target')\n",
        "        ax2.set_xlabel('Target')\n",
        "        ax2.set_ylabel(factor)\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    # 3. Machine Learning Feature Importance\n",
        "    print(\"\\n3Ô∏è‚É£ Machine Learning Feature Importance\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    try:\n",
        "        # Prepare data for ML\n",
        "        X = df_processed[feature_cols]\n",
        "        y = df_processed[target_col]\n",
        "        \n",
        "        # Train Random Forest\n",
        "        rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        rf.fit(X, y)\n",
        "        \n",
        "        # Get feature importance\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'feature': feature_cols,\n",
        "            'importance': rf.feature_importances_\n",
        "        }).sort_values('importance', ascending=False)\n",
        "        \n",
        "        print(\"Random Forest Feature Importance:\")\n",
        "        display(feature_importance)\n",
        "        \n",
        "        # Visualize feature importance\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.barplot(data=feature_importance, x='importance', y='feature', palette='viridis')\n",
        "        plt.title('Random Forest Feature Importance')\n",
        "        plt.xlabel('Importance Score')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error in ML analysis: {e}\")\n",
        "    \n",
        "    print(\"\\n‚úÖ Risk factor analysis completed!\")\n",
        "else:\n",
        "    print(\"‚ùå Cannot perform risk analysis - no target variable found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Key Insights and Recommendations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if df_processed is not None:\n",
        "    print(\"üéØ Key Insights and Recommendations\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Get dataset info\n",
        "    dataset_type = \"Diabetes\" if 'Outcome' in df_processed.columns else \"Heart Disease\"\n",
        "    total_records = len(df_processed)\n",
        "    \n",
        "    print(f\"üìä Dataset: {dataset_type}\")\n",
        "    print(f\"üìè Total Records: {total_records:,}\")\n",
        "    \n",
        "    # Get target info\n",
        "    target_cols = [col for col in df_processed.columns if any(t in col.lower() for t in ['outcome', 'target'])]\n",
        "    if target_cols:\n",
        "        target_col = target_cols[0]\n",
        "        disease_rate = (df_processed[target_col].sum() / len(df_processed)) * 100\n",
        "        print(f\"üéØ Disease Rate: {disease_rate:.1f}%\")\n",
        "    \n",
        "    print(\"\\nüîç Key Findings:\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    # Data quality insights\n",
        "    missing_values = df_processed.isnull().sum().sum()\n",
        "    duplicates = df_processed.duplicated().sum()\n",
        "    \n",
        "    print(f\"1. Data Quality:\")\n",
        "    print(f\"   ‚Ä¢ Missing values: {missing_values}\")\n",
        "    print(f\"   ‚Ä¢ Duplicate records: {duplicates}\")\n",
        "    print(f\"   ‚Ä¢ Data completeness: {((total_records - missing_values) / total_records) * 100:.1f}%\")\n",
        "    \n",
        "    # Feature insights\n",
        "    numerical_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
        "    feature_cols = [col for col in numerical_cols if col not in target_cols]\n",
        "    \n",
        "    print(f\"\\n2. Feature Analysis:\")\n",
        "    print(f\"   ‚Ä¢ Total features analyzed: {len(feature_cols)}\")\n",
        "    print(f\"   ‚Ä¢ Feature types: All numerical\")\n",
        "    \n",
        "    # Risk factor insights\n",
        "    if target_cols:\n",
        "        correlations = df_processed[feature_cols + [target_col]].corr()[target_col].drop(target_col).abs().sort_values(ascending=False)\n",
        "        top_risk_factors = correlations.head(3)\n",
        "        \n",
        "        print(f\"\\n3. Top Risk Factors:\")\n",
        "        for i, (factor, corr) in enumerate(top_risk_factors.items(), 1):\n",
        "            print(f\"   {i}. {factor}: {corr:.3f} correlation\")\n",
        "    \n",
        "    # Statistical insights\n",
        "    print(f\"\\n4. Statistical Insights:\")\n",
        "    for col in feature_cols[:3]:  # Show top 3 features\n",
        "        mean_val = df_processed[col].mean()\n",
        "        std_val = df_processed[col].std()\n",
        "        print(f\"   ‚Ä¢ {col}: Mean={mean_val:.2f}, Std={std_val:.2f}\")\n",
        "    \n",
        "    print(f\"\\nüí° Recommendations:\")\n",
        "    print(\"-\" * 30)\n",
        "    print(\"1. Data Collection:\")\n",
        "    print(\"   ‚Ä¢ Ensure regular data collection for all health metrics\")\n",
        "    print(\"   ‚Ä¢ Implement data validation to reduce missing values\")\n",
        "    print(\"   ‚Ä¢ Consider collecting additional lifestyle factors\")\n",
        "    \n",
        "    print(\"\\n2. Risk Assessment:\")\n",
        "    print(\"   ‚Ä¢ Focus on top risk factors identified in analysis\")\n",
        "    print(\"   ‚Ä¢ Implement regular monitoring for high-risk individuals\")\n",
        "    print(\"   ‚Ä¢ Develop early warning systems based on key indicators\")\n",
        "    \n",
        "    print(\"\\n3. Healthcare Strategy:\")\n",
        "    print(\"   ‚Ä¢ Create targeted intervention programs\")\n",
        "    print(\"   ‚Ä¢ Develop personalized risk profiles\")\n",
        "    print(\"   ‚Ä¢ Implement preventive care measures\")\n",
        "    \n",
        "    print(\"\\n4. Future Analysis:\")\n",
        "    print(\"   ‚Ä¢ Consider longitudinal studies for trend analysis\")\n",
        "    print(\"   ‚Ä¢ Explore machine learning models for prediction\")\n",
        "    print(\"   ‚Ä¢ Include demographic and lifestyle factors\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ Analysis completed successfully!\")\n",
        "    print(f\"üìà Use the Streamlit dashboard for interactive exploration\")\n",
        "    print(f\"üìä Run 'streamlit run app.py' to launch the dashboard\")\n",
        "else:\n",
        "    print(\"‚ùå Cannot generate insights - no processed dataset available\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
